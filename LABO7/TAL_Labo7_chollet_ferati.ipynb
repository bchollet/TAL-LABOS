{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cours TAL – Labo 7 : Classification de dépêches d’agence avec NLTK\n",
    "\n",
    "**Objectifs**\n",
    "L’objectif de ce labo est de réaliser des expériences de classification de documents avec la boîte à\n",
    "outils NLTK sur le corpus de dépêches Reuters. Le labo est à effectuer en binôme. Le rendu sera un\n",
    "notebook Jupyter présentant vos choix, votre code, vos résultats et les discussions. Le labo sera jugé\n",
    "sur la qualité des expériences et sur la discussion des différentes options explorées.\n"
   ],
   "id": "d05550221f1a3159"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Récupération des données\n",
    "\n",
    "**Données :** les dépêches du corpus Reuters, tel qu’il est fourni par NLTK. Veuillez respecter la\n",
    "division en données d’entraînement et données de test."
   ],
   "id": "71bdbbc171040120"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T11:49:12.515751Z",
     "start_time": "2024-06-04T11:48:51.165030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('reuters')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Nous faisons le choix de traiter tous les mots en minuscule\n",
    "documents = [(list(w.lower() for w in reuters.words(fileid)), category)\n",
    "             for category in reuters.categories()\n",
    "             for fileid in reuters.fileids(category)]\n",
    "\n",
    "documents_no_stop_words = [(list(w.lower() for w in filter(lambda w: w.lower() not in stop_words, reuters.words(fileid))), category)\n",
    "             for category in reuters.categories()\n",
    "             for fileid in reuters.fileids(category)]\n",
    "\n",
    "documents_lemmatized = [(list(w.lower() for w in map(lemmatizer.lemmatize, reuters.words(fileid))), category)\n",
    "             for category in reuters.categories()\n",
    "             for fileid in reuters.fileids(category)]"
   ],
   "id": "b5feb182f989dfe3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T12:38:53.714531Z",
     "start_time": "2024-06-04T12:38:48.457753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Récupère la fréquence des mots en filtrant les mots de moins de 4 caractères\n",
    "import re\n",
    "all_words = nltk.FreqDist(w.lower() for w in reuters.words() if re.match(r'^[a-z]{3,}$', w))\n",
    "# all_words = nltk.FreqDist(w.lower() for w in reuters.words())\n",
    "# On sélectionne les 2000 mots les plus fréquents comme features pour les classifieurs\n",
    "word_features = list(all_words)[:2000]"
   ],
   "id": "a5b19d4772d21d95",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T12:38:55.801076Z",
     "start_time": "2024-06-04T12:38:55.782260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# Modifie les catégories des documents pour les rendre binaires\n",
    "def documents_with_binary_category(documents, category):\n",
    "    return [(d, category if c == category else 'other') for (d, c) in documents]\n",
    "\n",
    "# split un dataset en 80% train et 20% test pour chaque category\n",
    "def split_dataset(documents):\n",
    "    dataset = {}\n",
    "    for (d, c) in documents:\n",
    "        if c not in dataset:\n",
    "            dataset[c] = []\n",
    "        dataset[c].append(d)\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for c in dataset:\n",
    "        random.shuffle(dataset[c])\n",
    "        train_set += [(d, c) for d in dataset[c][int(len(dataset[c]) * 0.2):]]\n",
    "        test_set += [(d, c) for d in dataset[c][:int(len(dataset[c]) * 0.2)]]\n",
    "    return train_set, test_set\n",
    "\n",
    "# Retourne un dictionnaire indiquant si une feature est présent dans le document\n",
    "def document_contains_features(document):\n",
    "    doc_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in doc_words)\n",
    "    return features\n",
    "\n",
    "def get_featuresets(documents):\n",
    "    return [(document_contains_features(d), c) for (d,c) in documents]\n",
    "\n",
    "# def train_classifier(documents):\n",
    "    # Mélange des documents pour ne pas récupérer les mêmes données de tests à chaque exécution\n",
    "    # random.shuffle(documents) \n",
    "\n",
    "    # featuresets = [(document_contains_features(d), c) for (d,c) in documents]\n",
    "    \n",
    "    # train_set, test_set = split_dataset(featuresets)\n",
    "    \n",
    "    # print la taille des sets pour chaque catégorie\n",
    "    # print('Train set size of money-fx:', len(list(filter(lambda x: x[1] == 'money-fx', train_set))))\n",
    "    # print('Train set size of other:', len(list(filter(lambda x: x[1] == 'other', train_set))))\n",
    "    # print('Test set size:', len(list(filter(lambda x: x[1] == 'money-fx', test_set))))\n",
    "    # print('Test set size:', len(list(filter(lambda x: x[1] == 'other', test_set))))\n",
    "\n",
    "    # classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    # print(nltk.classify.accuracy(classifier, test_set))\n",
    "    # classifier.show_most_informative_features(5)\n",
    "    # return classifier"
   ],
   "id": "b5a572e49a72875f",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Classifieurs binaires\n",
    "\n",
    "### 2.1 Classifieur binaire pour la catégorie 'money-fx'\n",
    "\n",
    "#### 2.1.1 Classifieur Bayésien naïf + lemmatisation"
   ],
   "id": "19ce11ccb64d7b14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T12:39:28.690903Z",
     "start_time": "2024-06-04T12:38:59.352659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_moneyfx_lemmatized = documents_with_binary_category(documents_lemmatized, 'money-fx')\n",
    "\n",
    "train_set_moneyfx_lemmatized, test_set_moneyfx_lemmatized = split_dataset(get_featuresets(dataset_moneyfx_lemmatized))\n",
    "\n",
    "classifier_moneyfx_lemmatized = nltk.NaiveBayesClassifier.train(train_set_moneyfx_lemmatized)"
   ],
   "id": "188ade5762dff722",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# calcul de la précision, du rappel et du f-score de classifier_moneyfx_lemmatized\n",
    "from nltk.metrics import precision, recall, f_measure\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set_moneyfx_lemmatized):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier_moneyfx_lemmatized.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('money-fx')\n",
    "\n",
    "print('Precision:', precision(refsets['money-fx'], testsets['money-fx']))\n",
    "print('Recall:', recall(refsets['money-fx'], testsets['money-fx']))\n",
    "print('F-Score:', f_measure(refsets['money-fx'], testsets['money-fx']))\n",
    "\n",
    "classifier_moneyfx_lemmatized.show_most_informative_features(5)"
   ],
   "id": "cf27b2aebbbb52f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
